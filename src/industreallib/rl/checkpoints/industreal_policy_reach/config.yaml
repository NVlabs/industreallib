task:
  env:
    numObservations: 17
    numActions: 6
train:
  params:
    seed: ${...seed}
    algo:
      name: a2c_continuous
    model:
      name: continuous_a2c_logstd
    network:
      name: actor_critic
      separate: false
      space:
        continuous:
          mu_activation: None
          sigma_activation: None
          mu_init:
            name: default
          sigma_init:
            name: const_initializer
            val: 0
          fixed_sigma: true
      mlp:
        units:
        - 256
        - 128
        - 64
        activation: elu
        d2rl: false
        initializer:
          name: default
        regularizer:
          name: None
    load_checkpoint: ${if:${...checkpoint},True,False}
    load_path: ${...checkpoint}
    config:
      name: ${resolve_default:IndustRealTaskReach,${....experiment}}
      full_experiment_name: ${.name}
      env_name: rlgpu
      multi_gpu: false
      ppo: true
      mixed_precision: true
      normalize_input: true
      normalize_value: true
      value_bootstrap: true
      num_actors: ${....task.env.numEnvs}
      reward_shaper:
        scale_value: 1.0
      normalize_advantage: true
      gamma: 0.99
      tau: 0.95
      learning_rate: 0.0001
      lr_schedule: fixed
      schedule_type: standard
      kl_threshold: 0.016
      score_to_win: 20000
      max_epochs: ${resolve_default:8192,${....max_iterations}}
      save_best_after: 50
      save_frequency: 100
      print_stats: true
      grad_norm: 1.0
      entropy_coef: 0.0
      truncate_grads: false
      e_clip: 0.2
      horizon_length: 512
      minibatch_size: 16384
      mini_epochs: 8
      critic_coef: 2
      clip_value: true
      seq_len: 4
      bounds_loss_coef: 0.0001
